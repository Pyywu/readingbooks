#无约束优化$min f(x)$
其算法很大程度上取决于$f$的性质
  我们可以毫不犹豫的说，我们能解决的问题，都是凸优化问题

  定义1 凸问题

  首先凸函数，$f(x + y) <= frac 1 2 (f(x) + f(y))$写成线性组合也行。是等价的。

  当然仅仅是凸函数是不行的，我们知道线性函数是凸的，但是线性函数在无约束下是无解的。

  固然我们需要另外的条件，下有界，$\exists lb , s.t. f(x) \ge lb \forall x$

  这里我们就有了有解的性质，或者说有有界解的性质

  然后我们需要了解解的基本性质
  一阶必要条件
  梯度为0
  二阶充分条件
  hession正定

  自然而然的我们就有了最速下降法，或者称梯度下降法
  对于当前点$x_k$，求梯度$\nabla f(x_k)$ 随后求步长，求解子问题 $min g(\alpha)= f(\alpha d_k + x_k)$

  一元函数求最优 求导数为0的点即可（一阶必要条件）

  然后我们根据步长来看就有一定的选法了， 上述是最速下降法， 可以证明是线性收敛的

  关于收敛速度我们有定义 $ \frac {\|x_{k+1}-x^*\|} {\|x_{k}-x^*\|^p}$为常数 则为p阶收敛

  这不是一个特别差的收敛速度，但是我们会去研究选取什么步长能加快或者不会导致不收敛，如果我们可以不去求解子问题，就能收敛，何乐而不为。事实上在机器学习当中我们就没有去求解过子问题。

  下面介绍收敛条件：




  我们在学习数值分析的时候知道由牛顿迭代法求解方程。
  那么我们的一阶必要条件不就是一个二阶方程么。同样我们用牛顿迭代法求解。
  思路就是考虑泰勒展开二阶就行。
  然后我们求解子问题二阶的
  $min f(x) = f(x_k) + (x-x_k)^T \nabla f(x_k) + \frac 12 (x-x_k)^T B_k (x-x_k) + O(\|(x-x_k)\|^3)$
  很自然我们有其最优点也就是柯西点$ x_{k+1}$解二次方程不用教把。
  牛顿法很直接，不用解子问题，只用解方程，但是二次方程需要知道其Henssion的信息。维度高了之后求Heesion爆难

  从而有拟牛顿法，我们不去每一步都求Hession。我们在上一步的B_k的基础上做个扰动就可以了
  比如说做秩一扰动，也就是拿一个1秩的矩阵去扰动。$B_{k+1} = B_k + g^Tg$
  SR1 : $B_{k+!} = B_k + \frac{(y_k - B_ks_k)(y_k -B_ks_k)^T}{(y_k-B_ks_k)^Ts_k}
  其不保证B_k的正定性，接近真实的Hesssion，一般用于信赖域法？

  
